<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Text Processing]]></title>
    <url>%2F2019%2F03%2F02%2FText%20preprocessing%2F</url>
    <content type="text"><![CDATA[Text preprocessingTokenizationTokenization is a step which splits longer strings of text into smaller pieces, or tokens. Larger chunks of text can be tokenized into sentences, sentences can be tokenized into words, etc.Tokenization in Keras: 1texts = ['The cat sat on the mat.','The dog ate my homework.'] 123from keras.preprocessing.text import Tokenizerfrom keras.preprocessing.sequence import pad_sequencesimport numpy as np 1234567# The max number of words in each samplemaxlen = 100training_samples = 200# Only pick the first max_words words up in tokenizationmax_words = 10000 Create a tokenizer 1tokenizer = Tokenizer(num_words=max_words) Build word index 1tokenizer.fit_on_texts(texts) Convert texts to a list of word indices 1sequences = tokenizer.texts_to_sequences(texts) Get word indices 1word_index = tokenizer.word_index 1word_index {&apos;ate&apos;: 7, &apos;cat&apos;: 2, &apos;dog&apos;: 6, &apos;homework&apos;: 9, &apos;mat&apos;: 5, &apos;my&apos;: 8, &apos;on&apos;: 4, &apos;sat&apos;: 3, &apos;the&apos;: 1} Pad sequences to same length 1data = pad_sequences(sequences,maxlen=maxlen) 1data array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 1, 5], [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 6, 7, 8, 9]]) Get shuffled word vectors1234indices = np.arange(data.shape[0])np.random.shuffle(indices)data = data[indices]labels = labels[indices] Split data into train and validation dataset1234x_train = data[:train_samples]y_train = labels[:train_samples]x_val = data[train_samples:train_samples + val_samples]y_val = labels[train_samples:train_samples + val_samples] Word VectorsOne-hot vectors1one_hot_results = tokenizer.texts_to_matrix(texts, mode='binary') Word embeddingsTwo ways to get word embeddings Get word embeddings for special task meanwhile training.At the begining, the vectors are random initalized vectors. Using pretrained vectors by others Build-in models with embedding layer1234from keras.layers import Embedding# The first param is the number of tokens, the second is embedding dimensionsembedding_layer = Embedding(1000, 64) 12from keras.datasets import imdbfrom keras import preprocessing 12345678max_features = 10000maxlen = 20(x_train, y_train), (x_test, y_test) = imdb.load_data(num_words=max_features)# pad sequences to same lengthx_train = preprocessing.sequence.pad_sequences(x_train, maxlen=maxlen)x_test = preprocessing.sequence.pad_sequences(x_test, maxlen=maxlen) 12from keras.models import Sequentialfrom keras.layers import Flatten, Dense, Embedding 123456model = Sequential()model.add(Embedding(max_features, 8, input_length=maxlen))model.add(Flatten())model.add(Dense(1, activation='sigmoid')) WARNING:tensorflow:From D:\Anaconda\envs\dl\lib\site-packages\tensorflow\python\framework\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version. Instructions for updating: Colocations handled automatically by placer. 12model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])model.summary() _________________________________________________________________ Layer (type) Output Shape Param # ================================================================= embedding_2 (Embedding) (None, 20, 8) 80000 _________________________________________________________________ flatten_1 (Flatten) (None, 160) 0 _________________________________________________________________ dense_1 (Dense) (None, 1) 161 ================================================================= Total params: 80,161 Trainable params: 80,161 Non-trainable params: 0 _________________________________________________________________ 1234histroy = model.fit(x_train,y_train, epochs=10, batch_size=32, validation_split=0.2) WARNING:tensorflow:From D:\Anaconda\envs\dl\lib\site-packages\tensorflow\python\ops\math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version. Instructions for updating: Use tf.cast instead. Train on 20000 samples, validate on 5000 samples Epoch 1/10 20000/20000 [==============================] - 3s 129us/step - loss: 0.6759 - acc: 0.6042 - val_loss: 0.6398 - val_acc: 0.6810 Epoch 2/10 20000/20000 [==============================] - 1s 66us/step - loss: 0.5657 - acc: 0.7428 - val_loss: 0.5467 - val_acc: 0.7206 Epoch 3/10 20000/20000 [==============================] - 1s 67us/step - loss: 0.4752 - acc: 0.7808 - val_loss: 0.5113 - val_acc: 0.7384 Epoch 4/10 20000/20000 [==============================] - 1s 65us/step - loss: 0.4263 - acc: 0.8079 - val_loss: 0.5008 - val_acc: 0.7454 Epoch 5/10 20000/20000 [==============================] - 1s 65us/step - loss: 0.3930 - acc: 0.8257 - val_loss: 0.4981 - val_acc: 0.7540 Epoch 6/10 20000/20000 [==============================] - 1s 64us/step - loss: 0.3668 - acc: 0.8394 - val_loss: 0.5013 - val_acc: 0.7534 Epoch 7/10 20000/20000 [==============================] - 1s 70us/step - loss: 0.3435 - acc: 0.8534 - val_loss: 0.5051 - val_acc: 0.7518 Epoch 8/10 20000/20000 [==============================] - 1s 66us/step - loss: 0.3223 - acc: 0.8658 - val_loss: 0.5132 - val_acc: 0.7486 Epoch 9/10 20000/20000 [==============================] - 1s 68us/step - loss: 0.3022 - acc: 0.8765 - val_loss: 0.5213 - val_acc: 0.7492 Epoch 10/10 20000/20000 [==============================] - 1s 70us/step - loss: 0.2839 - acc: 0.8860 - val_loss: 0.5302 - val_acc: 0.7466 Pretrained word vectorsUsing pretrained GloVe1f = open(os.path.join(glove_dir,'glove.6B.100d.txt')) 123456for line in f: values = line.split() word = values[0] coefs = np.asarray(values[1:],dtype='float32') embeddings_index[word] = coefsf.close() 1embedding_dim = 100 123456embedding_matrix = np.zeros((max_words,embedding_dim))for word, i in word_index.items()ï¼š if i &lt; max_words: embedding_vec = embeddings_index.get(word) if embedding_vec is not None: embedding_matrix[i] = embedding_vec Set embedding layer weights to pretrained vector matrixIf layer 0 is embedding layer in model 12model.layers[0].set_weights([embedding_matrix])model.layers[0].trainable = False]]></content>
      <tags>
        <tag>text</tag>
        <tag>preprocessing</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Hello World]]></title>
    <url>%2F2019%2F03%2F02%2Fhello-world%2F</url>
    <content type="text"><![CDATA[Welcome to Hexo! This is your very first post. Check documentation for more info. If you get any problems when using Hexo, you can find the answer in troubleshooting or you can ask me on GitHub. Quick StartCreate a new post1$ hexo new "My New Post" More info: Writing Run server1$ hexo server More info: Server Generate static files1$ hexo generate More info: Generating Deploy to remote sites1$ hexo deploy More info: Deployment]]></content>
  </entry>
</search>
